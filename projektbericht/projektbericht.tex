\documentclass[a4paper, 12pt]{scrartcl}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\linespread{1.2}
\parindent0cm
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{units}
\usepackage{mathtools}
\usepackage{subcaption}

\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\geometry{a4paper, top=25mm, left=20mm, right=20mm, bottom=30mm, headsep=10mm, footskip=12mm}
\usepackage{amssymb}
	
\lstset{literate=%
	{Ö}{{\"O}}1
	{Ä}{{\"A}}1
	{Ü}{{\"U}}1
	{ü}{{\"u}}1
	{ä}{{\"a}}1
	{ö}{{\"o}}1
	{█}{{$\blacksquare$}}1
	{~}{{\textasciitilde}}1
}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{deepred}{rgb}{0.6,0,0}

\lstset{ %
	backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
	basicstyle=\ttfamily\scriptsize,          % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,                 % sets automatic line breaking
	captionpos=b,                    % sets the caption-position to bottom
	commentstyle=\color{mygreen},    % comment style
	deletekeywords={...},            % if you want to delete keywords from the given language
	escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
	extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	frame=single,	                 % adds a frame around the code
	keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	keywordstyle=\color{blue},       % keyword style
	language=Python,                 % the language of the code
	morekeywords={*,...},            % if you want to add more keywords to the set
	emph={makevideo,checkgreen,checkblue,lenken,line,linienfahren,main},
	emphstyle=\color{deepred},
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=5pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
	rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces=false,          % underline spaces within strings only
	showtabs=false,                  % show tabs within strings adding particular underscores
	stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle=\color{mymauve},     % string literal style
	tabsize=2,	                     % sets default tabsize to 2 spaces
	title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}
 
\begin{document}
\thispagestyle{empty}

\begin{flushright}
	\includegraphics[width=5cm]{HAW_Logo.eps}
\end{flushright}

Hochschule für Angewandte Wissenschaften Hamburg\\
Fakultät Design, Medien und Information\\
Department Medientechnik\\[1ex]
IT-Systeme\\
Prof. Dr. Torsten Edeler

\vspace{1cm}

\begin{center}
	\large{Projektbericht}\\
	\LARGE{Raspberry Car --- Racingteam II}	
	\vspace{2ex}
	
	\large{
		Jörn Kogerup, 2248604\\
		Darius Weiberg, 2144123\\
		Mirco Hülsemann, 2248464
	}
	\vspace{2ex}
	
	\large{\today}
\end{center}

\vspace{1cm}

\begin{figure}[H] 
	\centering
	\makebox[\textwidth]{\includegraphics[width=\paperwidth]{Titel.png}}
	\caption{Auto von oben}
\end{figure}

\newpage

\tableofcontents

\newpage

\section{Einleitung}

Dieser Projektbericht dient der Dokumentation des Projekts „Entwicklung eines selbstfahrenden Autos“ im Modul IT-Systeme des Studiengangs Medientechnik an der HAW Hamburg im Wintersemester 2017/18.

Autonomes Fahren ist derzeit ein großes und hochaktuelles Forschungsthema vieler Universitäten, IT-Unternehmen und Automobilhersteller. Mit Hochdruck arbeiten Unternehmen wie Tesla, General Motors oder Waymo zusammen mit IT-Firmen wie Intel und Google daran, selbstfahrende Autos für die breite Masse verfügbar zu machen.

Aufgrund der aktuellen Relevanz und der Tatsache, dass ein selbstfahrendes Auto die ideale Verkörperung eines informationstechnischen Systems ist, wurde dieses Thema als Projekt für die Veranstaltung „IT-Systeme“ im Wintersemester 2017/2018 auserwählt.

Der vorliegende Abschlussbericht enthält Erläuterungen zum Projektziel, Projektumfeld sowie zu den gegebenen technischen Rahmenbedingungen. Es wird das technische Konzept zur Erreichung des Projektziels vorgestellt sowie die einzelnen Schritte des Entwicklungsprozesses dokumentiert und erläutert. Der gesamte Code sowie Bilder sind im Anhang des Dokuments zu finden.

\section{Konzept}

\subsection{Projektvorstellung}

Im Rahmen der Vorlesung IT-Systeme (5. Semester des Studiengangs Medientechnik) sollen die Studierenden in kleinen Gruppen ein autonomes Modellbau-Auto entwickeln, welches in der Lage sein soll, mithilfe von Sensoren und Kameras selbstständig eine vorher definierte Rennstrecke abzufahren. Die Aufgaben der Studierenden umfassen dabei:

\begin{itemize}
	\item eine konzeptionelle Planung zur Umsetzung des Projekts,
	\item die Konstruktion des Modellbau-Autos (dabei wird ein Bausatz verwendet) sowie der Rennstrecke,
	\item die Auseinandersetzung mit der Funktionsweise der elektrischen Komponenten,
	\item die Verschaltung der elektrischen Komponenten,
	\item Programmierung und Implementierung des Steuer-Codes auf einem Raspberry Pi.
\end{itemize}

Als zeitlicher Rahmen ist ein Semester vorgesehen. Pro Woche gibt es eine dreistündige Einheit, in der die Studierenden unter Betreuung am Projekt weiterarbeiten können. Es ist jedoch auch möglich, außerhalb dieser Einheit am Projekt weiterzuarbeiten.

Es gibt zudem zwei Challenge-Termine im Semester, zu welchen bestimmte Zwischenziele erreicht werden müssen. Auf diese Art wird das Vorankommen des Projekts überprüft und gewährleistet. Am Ende des Semesters erfolgt eine Generalprobe und schließlich die Projektausstellung.

\subsection{Projektziel}

Ziel des Projekts ist es, ein autonomes Auto zu entwickeln, welches mithilfe von Mikrocomputer, Kamera und Abstandssensoren zwei Runden auf einer definierten Rennstrecke selbstständig und erfolgreich absolviert. Dabei sollen die Autos einzeln starten, es befindet sich also nie mehr als ein Auto auf der Strecke. Die Gruppe, deren Auto den Kurs am schnellsten durchfährt, wird als Gewinner geehrt.

Auf dem Weg zu diesem Ziel gibt es im Laufe des Semesters zudem zwei Zwischenziele, sogenannte Challenges, die erfolgreich absolviert werden müssen. Das erste Zwischenziel ist eine Geradeausfahrt von 10 Metern mit Geschwindigkeitswechseln, das zweite Zwischenziel ist das Fahren entlang einer Wand mithilfe von Abstandssensoren.

\subsection{Die Rennstrecke}

\begin{wrapfigure}{L}{0.33\textwidth}
	\centering
	\includegraphics[width=0.3\textwidth]{Rennstrecke.png}
	\caption{Rennstrecke}
	\label{rennstrecke}
\end{wrapfigure}

Die Rennstrecke, die es zu meistern gilt, wird am Anfang des Semesters grob definiert:

Sie soll als geschlossene Rundstrecke (Startlinie = Ziellinie) in einem Laborraum der Hochschule aufgebaut werden. Im Wesentlichen soll sie aus einer Mittellinie sowie zwei Begrenzungslinien bestehen. Der Mittellinie gilt es zu folgen, die Begrenzungslinien dürfen nicht überschritten werden. Die Strecke soll außerdem mehrere Kurven mit verschiedenem Radius enthalten sowie einen geraden Abschnitt ohne Mittellinie entlang einer Wand. In diesem Abschnitt ohne Mittellinie sollen Abstandssensoren genutzt werden, um auf der Spur zu bleiben.

Die finale, endgültige Version der Rennstrecke wird im Laufe des Semesters anhand der oben genannten Richtlinien von einer Gruppe von Studierenden entworfen und sieht letztendlich wie folgt aus: 

Die Linien werden mit Klebeband realisiert, welches auf den Boden des Laborraums aufgeklebt wird. Als Mittellinie dient rotes Tape, als Begrenzungslinie schwarzes.

Aufgrund von Problemen mit den verfügbaren Abstandssensoren, die sich erst im Laufe des Semesters ergeben haben, ist der gerade Abschnitt entlang der Wand in der endgültigen Version nun doch mit einer Mittellinie versehen. Die Aufgabe, mithilfe von Abstandssensoren entlang einer Wand zu fahren, fällt somit -- zumindest in der Generalprobe und Projektausstellung -- weg.

Eine weitere Änderung der anfänglichen Version der Rennstrecke ist die Einführung einer Ampel an der Start- und Ziellinie. So sollen die Autos nur bei grünem Licht starten und automatisch anhalten, wenn das Licht blau geschaltet ist. Die Ampel wird durch einen LED-Streifen auf dem Boden realisiert.

\subsection{Das Auto}
Die Komponenten, die für den Bau des Autos verwendet werden dürfen, sind vorgegeben und werden von der Hochschule bereitgestellt:

\begin{itemize}
	\item Einfacher Modellbau-Auto-Bausatz, bestehend aus Rahmen, DC-Getriebemotoren und gummierten Rädern
	\item \unit[7,2]{V} NiMH Akku mit einer Kapazität von \unit[4000]{mAh}
	\item Motortreiber L298N für DC-Motoren mit Dual H-Bridge
	\item Raspberry Pi 3 Model B (4-Kern-CPU, \unit[1,2]{GHz}, \unit[1]{GB} Ram, WLAN, Bluetooth)
	\item USB-Webcam ($640\times 480$ Pixel, \unit[30]{fps}, manueller Fokus)
	\item Ultraschallsensor HC-SR04 (messbare Distanz: \unit[2--300]{cm}, Auflösung: \unit[3]{mm}, max. 50 Messungen pro Sekunde)
\end{itemize}

Alle Komponenten (mit Ausnahme des Raspberrys) wurden im Hinblick auf einen möglichst günstigen Einkaufspreis ausgewählt. Dementsprechend sind die Erwartungen an Haltbarkeit, Genauigkeit und Ausfallsicherheit der Produkte und letztendlich an die Performance des Autos nicht allzu hoch.

Neben den aufgeführten Komponenten steht den Studierenden eine Werkstatt mit 3D-Drucker, Lötgerät und einer Vielzahl an Elektronik-Bauteilen (Kabel, Widerstände, Platinen \ldots) zur Verfügung.

Als Programmiersprache wird Python verwendet. Diese universelle Sprache zeichnet sich durch einfache und leicht verständliche Syntax, große Nutzerbasis sowie mächtige Bibliotheken aus und ist quasi Standardsprache auf dem Raspberry Pi.

\subsection{Umsetzung}

\begin{figure}[H] 
	\centering
	\begin{subfigure}{.5\textwidth} 
		\centering
		\includegraphics[width=\textwidth]{../bilder_videos/RaspberryCar.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth} 
		\centering
		\includegraphics[width=\textwidth]{../bilder_videos/RaspberryCar-2.png}
	\end{subfigure}%
	\caption{Das Auto im finalen Aufbau}
	\label{auto}
\end{figure}

Schon seit Beginn des Projektes sollten alle Bestandteile des Autos kompakt verbaut werden. Die Bauteile wurden fest angebracht und sind so angeordnet, dass sie das Gewicht des Autos gleichmäßig verteilen. Auf besondere äußerliche Ausstattungsmerkmale wurde deshalb auch bewusst verzichtet, um mehr Geschwindigkeit zuzulassen und die Motoren weniger zu belasten.

Da die Rennstrecke im Wesentlichen daraus bestand, einer roten Linie zu folgen, haben wir uns bereits früh mit der Bildverarbeitung beschäftigt. Dabei wurde in der Gruppe parallel an der Wandfahrt mit den Ultraschallsensoren und an der Linienfahrt mit der Kamera gearbeitet.

Nachdem das Grundgerüst für die Linienfahrt innerhalb weniger Wochen erarbeitet wurde, gab es genug Zeit für die Optimierung des Algorithmus. Die Programmierung wurde so angegangen, alle Kurven zu durchfahren, ohne die Geschwindigkeit des Autos zu vernachlässigen.

\ldots

\subsection{Schaltplan}

\begin{figure}[H] 
	\centering
	\includegraphics[width=.5\textwidth]{Schaltplan_Steckplatine.png}
	\caption{Schaltplan}
	\label{schaltplan}
\end{figure}

\ldots

\section{Code}

Das Programm wird über die Datei \textit{main.py} ausgeführt. Zwei Module \textit{setup.py} und \textit{aufrauemen.py} sind ausgelagert, dadurch ist die Hauptdatei übersichtlicher und die Module können einfacher von anderen Dateien mitbenutzt werden.

\subsection{main.py}

In \textit{main.py} wird das Modul Threading verwendet, was gleich mehrere Vorteile hat. Die Bilderaufnahme ist extrem viel schneller, wenn sie auf einem anderen Thread als der restliche Algorithmus läuft und das Abspeichern der Bilder für eine Videoausgabe würde ohne das Threading die Rechendauer um ein Vielfaches erhöhen. Außerdem können so andere Funktionen unabhängig von dem restlichen Programm problemlos ausgeführt werden.

Es laufen drei Threads parallel. Der eigentliche Fahralgorithmus \textit{linienfahren}, die Videoausgabe \textit{makevideo} und die Ampelüberprüfung \textit{checkblue}. \textit{linienfahren} ruft zwei Funktionen auf, \textit{line} zur Bildaufnahme und \textit{lenken} zur Motorsteuerung.

Es werden mehrere globale Variablen zwischen den einzelnen Threads benutzt. Zum einen die Bildvariablen \textit{ret} und \textit{img}, damit nicht jede Funktion eigene Bilder aufnehmen muss und zum anderen der Linienmesswert \textit{x} und die verstrichene Zeit \textit{minutes, seconds}, die zur Videoausgabe benötigt werden.

\subsubsection{import}
\lstinputlisting[language=Python,firstline=1,lastline=14,caption={}]{../main.py}

Am Anfang der Hauptdatei werden die benötigten Module importiert, gefolgt von den beiden ausgelagerten Dateien. Von \textit{aufrauemen.py} werden die benötigten Funktionen und von \textit{setup.py} wird alles importiert, da diese Datei keine Funktion enthält.

In den Zeilen 10 bis 14 werden die benötigten Einstellungen für OpenCV 2 vorgenommen und ein erstes Bild aufgenommen.

\subsubsection{main}
\lstinputlisting[language=Python,firstline=167,caption={}]{../main.py}

Das Programm wird in \textit{main} gestartet und beendet. Als erstes werden die Motoren mit der Funktion \textit{losfahren} gestartet, allerdings noch mit einem Tastverhältnis von \unit[0]{\%}.
Anschließend wird das Threading gestartet. Dafür wird \textit{run\_event}, das als \textit{while}-Bedingung für die Threads dient, gesetzt. Die Delay-Angaben und die drei verwendeten Threads \textit{th1}, \textit{th2} und \textit{th3} werden definiert. Dabei ist \textit{target} die Funktion, die in dem neuen Thread laufen soll und \textit{args} sind die Variablen, die mit übergeben werden.
Die drei Threads sind \textit{linienfahren} (der eigentliche Lenk-Algorithmus), \textit{checkblue} (Test, ob die Ampel blau leuchtet) und \textit{makevideo} (die Videoausgabe).

Der erste Thread (die Videoaufzeichnung) wird gestartet, gefolgt von einer \textit{while}-Schleife, die überprüft, ob die Ampel grün ist. Wenn der zurückgegebene Wert der \textit{checkgreen}-Funktion größer gleich 500 ist, also ausreichend Grünanteile erkannt wurden, endet die Schleife. Dadurch werden die anderen beiden Threads gestartet, wodurch das Auto losfährt.

Damit das Programm wieder beendet werden kann, folgt eine \textit{try-while}- und \textit{except}-Schleife, die weiter keine andere Funktion hat. Sobald ein \textit{KeyboardInterrupt} durch das Drücken von Strg-C auftritt, wird das Auto gebremst und die Threads werden geschlossen. Allerdings muss dabei gewartet werden, bis die \textit{while}-Schleifen aller Threads durchgelaufen sind. Anschließend werden die GPIO-Einstellungen bereinigt, die Videoaufnahme beendet und das Video gespeichert.

Gestartet wird die \textit{main}-Funktion durch eine Abfrage \textit{if \_\_name\_\_ == "\_\_main\_\_"}, um die Datei auch als Modul verwenden zu können.

\subsubsection{line}
\lstinputlisting[language=Python,firstline=104,lastline=119,caption={}]{../main.py}

In \textit{line} wird ein Bild aufgenommen und in einer Zeile der Mittelpunkt der roten Anteile ermittelt.

Der Funktion wird beim Aufrufen die Zeile (\textit{zeileNr}) übergeben, die auf den Rotwert analysiert werden soll. Damit wird es ermöglicht, dieselbe Funktion zum Analysieren unterschiedlicher Zeilen zu verwenden. Das wird in der finalen Version aber nicht benutzt.

In Zeile 12 bis 16 wird der Mittelpunkt berechnet und zurückgegeben. Falls kein Rotanteil über dem Schwellwert liegt, wird \textit{None} zurückgegeben.

\ldots

\subsubsection{linienfahren}
\lstinputlisting[language=Python,firstline=122,lastline=164,caption={}]{../main.py}

\textit{linienfahren} ruft die Funktion \textit{line} auf und berechnet die Lenkung und Geschwindigkeit aus dem zurückgegebenen Wert.

Am Anfang der Funktion wird ein Testbild aufgenommen und die für die Berechnung benötigten Variablen erstellt. Danach folgt eine \textit{while}-Schleife, die bis zum Beenden des Threading durchläuft.
Falls keine rote Linie im Bild war, oder in seltenen Fällen, wenn die Linie nicht erkannt wurde, gibt \textit{line} \textit{None} zurück. In diesem Fall wird in den Zeilen 17 bis 21 überprüft, ob die Linie zuletzt rechts oder links im Bild war. Der Messwert \textit{mitte} wird dementsprechend auf das Minimum (0) oder das Maximum (640) gesetzt. Anschließend wird der Wert für die Videoausgabe abgespeichert (Zeile 22).

In den Zeilen 24 bis 33 wird der Wert für die Lenkung und Geschwindigkeit berechnet. Unterschieden wird zwischen drei Fällen. Ist der Messwert in der Mitte des Bildes, wird geradeaus gelenkt, andernfalls wird die Lenkung und die Geschwindigkeit berechnet. Ist der Messwert größer als der Bildmittelpunkt, wird $2-\textit{Lenkwert}$ gerechnet. Es ergibt sich ein Lenkwert zwischen 0 und 1 für eine Linkskurve und 1 bis 2 für eine Rechtskurve. Das hat den Vorteil, dass der \textit{lenken}-Funktion nicht zusätzlich eine Richtungsangabe übergeben werden muss. Anschließend wird die \textit{lenken}-Funktion mit den Lenk- und Geschwindigkeits-Variablen aufgerufen.

Der Wert für die Lenkung und die Geschwindigkeit wird aus dem Messwert und dem Bildmittelpunkt berechnet:
\begin{align}
\text{Lenkung}&=\frac{\text{Messwert}}{\text{Bildmittelpunkt}}\cdot\unit[90]{\%}+\unit[10]{\%}\\[2ex]
\text{Geschwindigkeit}&=\frac{\text{Messwert}}{\text{Bildmittelpunkt}}\cdot\unit[60]{\%} + \unit[40]{\%}
\end{align}
Damit die Lenkung nicht zu stark ist, wird der Wert kleiner gewichtet ($\cdot\unit[90]{\%}$) und angehoben ($+\unit[10]{\%}$). So startet der Wert nicht bei \unit[0]{\%}, sondern bei \unit[10]{\%}.
Die Geschwindigkeit wird gleich berechnet, allerdings mit ($\cdot\unit[60]{\%}$) gewichtet und ($+\unit[40]{\%}$) angehoben. Es wird also mit \unit[60]{\%} bis \unit[100]{\%} Geschwindigkeit gefahren.

Für die Text- und Videoausgabe wird die verstrichene Zeit berechnet. Anschließend folgt in Zeile 40/41 eine Textausgabe über die Konsole. Dabei werden $\frac{\text{Messwert}}{10}$ Leerzeichen, dann ein Blockzeichen als Position des Messpunktes im Bild und die verbleibenden $64-\frac{\text{Messwert}}{10}$ Leerzeichen geschrieben. Dann folgen noch Angaben zu Messpunkt, Lenkwert, Geschwindigkeit und Zeit. In der nächsten Zeile wird der Mittelpunkt mit einem Strich markiert. Es ergibt sich eine Textausgabe, die zum Beispiel so aussieht:

\begin{lstlisting}[numbers=none]
               █                     x = 245.0 ;steer = 0.8 ;speed = 85.9 ;time = 00:00.00
                       |
                 █                   x = 265.0 ;steer = 0.8 ;speed = 89.7 ;time = 00:00.04
                       |
                   █                 x = 284.0 ;steer = 0.9 ;speed = 93.2 ;time = 00:00.08
                       |
                    █                x = 292.0 ;steer = 0.9 ;speed = 94.8 ;time = 00:00.12
                       |
                     █               x = 304.0 ;steer = 1.0 ;speed = 97.0 ;time = 00:00.16
                       |
                      █              x = 312.0 ;steer = 1.0 ;speed = 98.5 ;time = 00:00.20
                       |
                       █             x = 325.0 ;steer = 1.0 ;speed = 99.1 ;time = 00:00.24
                       |
\end{lstlisting}

\subsubsection{lenken}
\lstinputlisting[language=Python,firstline=76,lastline=101,caption={}]{../main.py}

Die \textit{lenken}-Funktion steuert das Tastverhältnis der Motoren. Übergeben werden zwei Parameter für die Lenkgewichtung und die Geschwindigkeit. Am Anfang der Funktion werden einige Abfragen zur Fehlerverhütung vorgenommen, damit das Tastverhältnis nicht unter \unit[0]{\%} oder über \unit[100]{\%} liegt, das würde sonst zu einem Absturz führen.

Damit das Auto in den Kurven nicht ungewollt langsamer fährt, wird ausgerechnet, wie viel schneller sich die äußeren Motoren drehen können.
\begin{align}
\text{Kopfraum} = 100-\text{Geschwindigkeit}
\end{align}
Ist der Kopfraum größer als der Geschwindigkeitswert, wird der Kopfraum gleich der Geschwindigkeit gesetzt.

Die Tastverhältnisse der Motoren werden gleich dem Geschwindigkeitswert gesetzt, wenn der Lenkwert $steer=1$ ist. Sonst wird unterschieden, ob der Lenkwert größer oder kleiner als 1 ist, um in die entsprechende Richtung zu lenken.
\begin{align}
	&\text{Innere Motoren:} &\text{Tastverhältnis} &= \text{Lenkwert}\cdot\text{Geschwindigkeit}\\
	&\text{Äußere Motoren:} &\text{Tastverhältnis} &= (1-\text{Lenkwert})\cdot\text{Kopfraum}+\text{Geschwindigkeit}
\end{align}
Die Kombination aus dem Lenkwert, Geschwindigkeitswert und Kopfraum ermöglicht ein stufenloses Kurvenfahren, ohne dass an Geschwindigkeit verloren wird. Dabei ist es egal, worauf die Werte basieren, es wird immer gleich sanft gelenkt.

\begin{table}[H]
	\centering
	\begin{tabular}{c|c|c|c}
		Lenken & Geschwindigkeit & $T_{\text{innen}} [\%]$ &  $T_{\text{außen}}[\%]$ \\ \hline 
		0,2 & 20 & \ 2 & 36 \\  
		    & 60 & 12  & 92 \\    
		    & 80 & 16  & 96 \\ \hline
		0,8 & 20 & 16  & 24 \\
		    & 60 & 48  & 68 \\
		    & 80 & 64  & 84 \\
	\end{tabular} 
	\caption{Beispielwerte für die Motorensteuerung}
\end{table}

\subsubsection{checkgreen}
\lstinputlisting[language=Python,firstline=36,lastline=50,caption={}]{../main.py}

Die \textit{checkgreen}-Funktion ist eine einfache Methode zur Erkennung einer grünen Ampel. Sie analysiert ein ganzes Bild der Webcam auf ihren Grünanteil und gibt die Anzahl der Pixel in einem gewählten HSV-Bereich an. Dabei ist zu beachten, dass die Funktion nur auf die Farbe der Ampel reagiert und nicht einen Fehlstart des Autos durch andere Grüntöne im Bild auslöst. Um dies zu vermeiden, wird eine Umwandlung des Farbraums von BGR -- was die Kamera nativ ausgibt -- zu HSV vorgenommen, um einen präzisen Farbbereich mit zwei Threshold-Werten für die untere und obere Grenze mithilfe von OpenCV festzulegen. Danach wird eine Maske mit der Funktion \textit{cv2.inRange} erstellt, in der alle Pixel innerhalb des Farbbereichs liegen.
Um festzustellen, dass die Ampel im Bild grün ist, werden alle weißen Pixel der Maske mit \textit{cv2.countNonZero} gezählt. Erst bei einer genügend großen Anzahl an grünen Pixeln, bei uns mehr als 500, startet das Auto die Fahrt.

Der HSV-Raum eignet sich im Gegensatz zu BGR gut für die Selektion eines Farbbereiches, weil er sich aus drei Komponenten Hue (Farbton), Saturation (Farbsättigung) und Value (Hellwert) zusammensetzt. Als Testbild wurde mit der Webcam ein Bild des LED-Streifens aufgenommen, wobei zusätzlich noch ein ähnlicher Farbton in Form eines grünen Klebebandes dazukommt, um die Grenzen besser festzulegen. Für die Umrechnung von BGR- nach HSV-Werten gelten je nach Formelsatz andere Intervalle. Die Tabelle \ref{hsv} zeigt einen Vergleich zwischen gängigen Bildbearbeitungsprogrammen und OpenCV:

\begin{table}[H]
	\centering
	\begin{tabular}{l|l|l}
		Komponente & Bildbearbeitungsprogramm & OpenCV
		\\
		\hline
		Hue (Farbton) & $H\in\left[0^\circ, 360^\circ\right]$ & $H\in\left[0^\circ,179^\circ\right]$
		\\
		Saturation (Farbsättigung) & $S\in\left[0, 1\right]$ & $S\in\left[0, 255\right]$
		\\
		Value (Hellwert) & $V\in\left[0, 1\right]$ & $V\in\left[0, 255\right]$
	\end{tabular} 
	\caption{Intervalle des HSV-Farbraums}
	\label{hsv}
\end{table}

Eine schnelle Umrechnung von BGR (Werte zwischen 0 und 255) nach HSV lässt sich durch folgende Befehle umsetzen:

\begin{lstlisting}
import cv2
import numpy as np
bgr_color = np.uint8([[[B,G,R]]])
hsv_color = cv2.cvtColor(bgr_color,cv2.COLOR_BGR2HSV)
print(hsv_color)
\end{lstlisting}

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../testbilder/gruen.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../testbilder/gruen2.png}
	\end{subfigure}
	\vspace{1ex}
	
	\includegraphics[width=1\textwidth]{../testbilder/Gruen_Bereich.pdf}
	\caption{Erkennung der Grünanteile}
\end{figure}

Nach der Funktion werden 2616 grüne Pixel für den gewählten Bereich gezählt, und zwar nur das von den LEDs erzeugte Licht und nicht das grüne Klebeband. Es war auch darauf zu achten, dass unser Auto nur bei einer grünen und nicht auch bei einer blauen Ampel losfährt, da die LEDs auf dem Kamerabild teilweise ins Türkis und Hellgrün hineinragen.

%Anfangs gab es eine andere Herangehensweise. Wir wollten die Formen der einzelnen LEDs durch Objekterkennung benutzen, um die grüne Ampel zu erkennen. Leider sind die LEDs für die Kamera so hell, dass nur die umliegenden Bereiche die Farbe abstrahlen und die Lichtquelle komplett weiß ist. So konnten wir eine grüne Ampel nicht von einer roten Ampel nur durch die Anzahl und Größe der Kreise unterscheiden.

\subsubsection{checkblue}
\lstinputlisting[language=Python,firstline=53,lastline=73,caption={}]{../main.py}

\textit{checkblue} funktioniert ähnlich wie \textit{checkgreen}, allerdings läuft die Funktion in Dauerschleife und stoppt das Auto nach 1,5 Sekunden, nachdem die blaue Ampel erkannt wurde. Dies hat den einfachen Zweck, dass das Auto nicht sofort bei Erkennung der blauen Ampel vor der Ziellinie stehen bleibt, sondern diese sicher überschreiten kann. Die Funktion nimmt dabei kein eigenes Bild auf, sondern verwendet das Bild, das von der \textit{linienfahren}-Funktion aufgenommen wurde. Beim Starten des Threads wartet die Funktion eine Sekunde, damit die grüne Ampel nicht fälschlicherweise als Stoppsignal erkannt wird.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../testbilder/blau.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../testbilder/blau2.png}
	\end{subfigure}
	\vspace{1ex}

	\includegraphics[width=1\textwidth]{../testbilder/Blau_Bereich.pdf}
	\caption{Erkennung der Blauanteile}
\end{figure}


\subsubsection{makevideo}
\lstinputlisting[language=Python,firstline=17,lastline=33,caption={}]{../main.py}

Die Aufgabe der \textit{makevideo}-Funktion ist es, die von der Kamera aufgenommenen Bilder als ein Video zu exportieren. Da das Video nur zur Fehleranalyse und Veranschaulichung dient, wird es vernachlässigt, ob das Video in Echtzeit läuft. Dafür wird die aktuelle Fahrzeit, ab dem Erkennen der grünen Ampel, unten links im Bild eingeblendet. Außerdem wird der zu dem Bild gehörende Messpunkt und dessen Wert eingezeichnet.

Das Video in Zusammenhang mit den eingeblendeten Werten ermöglicht eine deutlich bessere Fehleranalyse als eine Textausgabe über die Konsole (Abb. \ref{schuh_im_bild}).

\begin{figure}[H] 
	\centering
	\includegraphics[width=.5\textwidth]{schuh_im_bild.png}
	\caption{Kamerabild mit Zeitanzeige und Messpunkt, abgelenkt von einem roten Schuh}
	\label{schuh_im_bild}
\end{figure}

\subsection{setup.py}
\lstinputlisting[language=Python,caption={}]{../setup.py}

In \textit{setup.py} werden die GPIOs definiert, das Skript wird am Anfang von \textit{main.py} aufgerufen. Verwendet wird der BCM-Modus. Für die PWM wurde eine Frequenz von \unit[73]{Hz} gewählt, was auch mit den Ultraschallsensoren zusammenhing.

Dadurch, dass das GPIO-Setup ausgelagert war, konnten alle Python-Skripte während der ganzen Entwicklungsphase problemlos verwendet werden. Wenn die GPIOs sich ändern, muss dies nur in dieser einen Datei eingetragen werden.

\subsection{aufraeumen.py}
\lstinputlisting[language=Python,caption={}]{../aufraeumen.py}

In \textit{aufrauemen.py} sind drei Funktionen ausgelagert. \textit{losfahren} setzt die vier GPIOs der Motoren auf die entsprechenden Werte zum Vorwärtsfahren, \textit{bremsen} setzt die GPIOs zum Stoppen des Autos auf Null und \textit{aufrauemen} stoppt das Auto und bereinigt die GPIO-Einstellungen.

\section{3D-Druck-Halterungen}

\subsection{Kamerahalterung}
\begin{figure}[H] 
	\centering
	\begin{subfigure}{.33\textwidth} 
		\centering
		\includegraphics[width=\textwidth]{../3D-Druck_Modelle/Kamerahalterung_neu.png}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth} 
		\centering
		\includegraphics[width=\textwidth]{../3D-Druck_Modelle/Kamerahalterung_neu_top.png}
	\end{subfigure}%
	\begin{subfigure}{.33\textwidth} 
		\centering
		\includegraphics[width=\textwidth]{../bilder_videos/Kamerahalterung.png}
	\end{subfigure}%
	\caption{Kamerahalterung 3D-Modell und fertiger 3D-Druck}
	\label{kamerahalterung}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=.5\textwidth]{../3D-Druck_Modelle/Kamerahalterung_alt.png}
	\caption{3D-Modell der ersten Kamerahalterung}
	\label{kamerahalterung alt}
\end{figure}

\subsection{Ultraschallsensorhalterung}

Für den Ultraschallsensor HC-SR04 wurde eine Halterung entworfen, die es ermöglicht, den Sensor sowohl aufrecht als auch kopfüber an dem Auto zu befestigen. Außerdem sollte der Sensor, wie alle Teile des Autos, einfach abnehmbar sein, falls Änderungen vorgenommen werden müssen. Entworfen wurde das Modell in Blender 2.79 mit zwei Löchern für Schrauben, Platz für weitere Bohrungen und einen Schlitz für die Steckverbindung.

Der 3D-Druck ging problemlos und die Halterung erfüllte die Anforderungen, es stellte sich aber heraus, dass die Maße der Sensoren teilweise zu stark voneinander abweichen, deswegen wurde das Modell noch einmal mit mehr Spielraum überarbeitet.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../3D-Druck_Modelle/Sensorhalterung.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../3D-Druck_Modelle/Sensorhalterung_Top.png}
	\end{subfigure}\\
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../bilder_videos/Sensorhalterung.png}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../bilder_videos/Sensorhalterung_Top.png}
	\end{subfigure}%
	\caption{Ultraschallsensorhalterung 3D-Modell und fertiger 3D-Druck}
\end{figure}

\section{Probleme}
Der günstige Anschaffungspreis des Autobausatzes führte zu einem schnellen Verschleiß der linken Motoren, die wir vor der Prüfung beide austauschen mussten, da sie durch die fast ausschließlich vorkommenden Rechtskurven stärker beansprucht werden, als die rechten Motoren.

Wie in allen anderen Gruppen machte der Ultraschallsensor nach einiger Zeit Probleme, was allerdings nicht nur auf die minderwertige Qualität zurückzuführen ist. Meistens waren die gemessenen Abstände zur Wand bei Stillstand des Autos bis auf wenige Abweichungen im Toleranzbereich. Erst bei der Fahrt war die konstante Einhaltung des Abstandes zur Wand zunächst in vielen Fällen nicht zuverlässig, da die Messwerte aus unbekannten Gründen völlig in die Höhe gestiegen sind, obwohl die Distanz nahezu identisch geblieben ist. Dies wurde aber auch durch die Pulsweitenmodulation der Motoren beeinträgt, dessen Frequenzen der Rechteckschwingungen mit dem Senden des Triggers und Empfangen des Echos am Sensor zu Kollisionen führte. Nach einer Anpassung der Frequenz von \unit[70]{Hz} auf \unit[73]{Hz} funktionierte die Wandfahrt schon deutlich besser und wurde um einiges zuverlässiger. Für zukünftige Projekte wäre es ratsam, auf hochwertigere Ultraschallsensoren umzusteigen und einen Aufpreis in Kauf zu nehmen, aber dafür ein stabileres Fahrverhalten zu gewährleisten.

\section{Ergebnis}

Am Tag der Präsentation konnten wir unsere persönliche Bestzeit vom Vortag nochmals um 15 Sekunden verbessern. Pro Runde benötigte unser Auto durchschnittlich 52 Sekunden, also insgesamt 1:44 Minuten für zwei Runden. Die gesamte Strecke wurde völlig autonom bewältigt, denn unser Auto fuhr problemlos bei der grünen Ampel los und beendete seine Fahrt kurz hinter der Ziellinie nach zwei Runden durch die blaue Ampel. Während der Fahrt befand sich das Auto zu jeder Zeit innerhalb der schwarzen Begrenzungslinien und musste an keiner Stelle neu auf die Strecke gesetzt werden.

Der Abstand von 48 Sekunden zum zweitschnellsten Team ist dadurch zu erklären, dass die Motoren an keiner Stelle mit weniger als \unit[60]{\%} der Höchstgeschwindigkeit betrieben wurden und das Auto selbst in den engsten Kurven immer noch sehr zügig der Linie gefolgt ist, ohne sie dauerhaft zu verlieren und vom Kurs abzukommen. Das Verhältnis zwischen sicherem Fahren und hoher Geschwindigkeit wurde auf dem Raspberry Pi durch unseren Code ideal ausgenutzt.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Racing_Infos.jpeg}
	\caption{Gesamtzeiten aller ITS-Gruppen}
\end{figure}


\section{Geplante Weiterentwicklungen}

Es sind diverse Weiterentwicklungen an dem Auto geplant, die bisher nicht verwirklicht wurden.
Ein Plan ist es, die Videoausgabe deutlich auszubauen. So können mehr Messwerte angezeigt werden, wie zum Beispiel die Lenk- und Geschwindigkeitswerte oder die Drehzahl der Motoren und damit die zurückgelegte Strecke. Optimal wäre die Ausgabe eines Livestreams, was allerdings nur möglich ist, wenn dadurch der Fahralgorithmus nicht verlangsamt wird.
Zudem war eine Anzeige der aktuellen Messwerte über LEDs in Entwicklung.

Der Aufbau der Bauteile auf dem Auto soll noch kompakter werden, in dem platzsparender gelötet wird. Dadurch wären alle Platinen auf der unteren Platte platziert, sodass nur noch der Raspberry Pi, der Akku und die Kamera zu sehen sind.

Es ist auch eine andere Herangehensweise an die Programmieren mithilfe von neuronalen Netzen denkbar. So würde das Auto mit der Zeit selbstständig lernen, wie es die Rennstrecke zu absolvieren hat und wie die Schwierigkeiten, zum Beispiel enge Kurven, am besten gehandhabt werden können. Danach wäre es auch möglich, wie im Straßenverkehr vorausschauend zu fahren und mögliche Gefahren und Hindernisse frühzeitig zu entdecken und dagegen vorzugehen. Nichtsdestotrotz werden damit Kenntnisse in der Programmieren vorausgesetzt, die wahrscheinlich nicht vollständig innerhalb eines Semesters erlernt werden können.

\newpage

%Tabellenverzeichnis
\listoftables
\addcontentsline{toc}{section}{Tabellenverzeichnis} 

%Abbildungsverzeichnis
\listoffigures
\addcontentsline{toc}{section}{Abbildungsverzeichnis} 

%Literaturverzeichnis
\renewcaptionname{ngerman}{\refname}{Literaturverzeichnis}
\addcontentsline{toc}{section}{Literaturverzeichnis} 

\begin{thebibliography}{}
	\bibitem{}
	Edeler, Torsten: \textit{IT-Systeme}, HAW Hamburg, 13. Oktober 2017
\end{thebibliography}
\end{document}
